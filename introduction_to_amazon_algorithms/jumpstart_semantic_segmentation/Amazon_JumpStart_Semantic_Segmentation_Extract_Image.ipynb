{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation: How to run inference on the endpoint you have created?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img_pedestrian.png\" alt=\"Pedestrian Image\" style=\"height: 600px;\"/><figcaption>img_pedestrian.png</figcaption>The image has been downloaded from https://www.cis.upenn.edu/~jshi/ped_html/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "s3_bucket = f\"jumpstart-cache-prod-{region}\"\n",
    "key_prefix = \"inference-notebook-assets\"\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def download_from_s3(key_filenames):\n",
    "    for key_filename in key_filenames:\n",
    "        s3.download_file(s3_bucket, f\"{key_prefix}/{key_filename}\", key_filename)\n",
    "        \n",
    "img_jpg = \"img_pedestrian.png\"\n",
    "download_from_s3(key_filenames=[img_jpg])\n",
    "\n",
    "HTML('<img src=\"img_pedestrian.png\" alt=\"Pedestrian Image\" style=\"height: 600px;\"/>'\n",
    "     '<figcaption>img_pedestrian.png</figcaption>The image has been downloaded from '\n",
    "     'https://www.cis.upenn.edu/~jshi/ped_html/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query endpoint that you have created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def query_endpoint(input_img):\n",
    "    endpoint_name = 'jumpstart-dft-mx-semseg-fcn-resnet101-coco'\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/x-image', Body=input_img, Accept='application/json;verbose')\n",
    "    return response\n",
    "\n",
    "def parse_response(query_response):\n",
    "    response_dict = json.loads(query_response['Body'].read())\n",
    "    return response_dict['predictions'],response_dict['labels'], response_dict['image_labels']\n",
    "\n",
    "with open(img_jpg, 'rb') as file: input_img = file.read()\n",
    "\n",
    "try:\n",
    "    query_response = query_endpoint(input_img)\n",
    "except Exception as e:\n",
    "    if e.response['Error']['Code'] == 'ModelError':\n",
    "        raise Exception(\n",
    "             \"Backend scripts have been updated in Feb '22 to standardize response \"\n",
    "             \"format of endpoint response.\"\n",
    "             \"Previous endpoints may not support verbose response type used in this notebook.\"\n",
    "             f\"To use this notebook, please launch the endpoint again. Error: {e}.\"\n",
    "        )\n",
    "    else:\n",
    "        raise\n",
    "try:\n",
    "    predictions, labels, image_labels =  parse_response(query_response)\n",
    "except (TypeError, KeyError) as e:\n",
    "    raise Exception(\n",
    "          \"Backend scripts have been updated in Feb '22 to standardize response \"\n",
    "          \"format of endpoint response.\"\n",
    "           \"Response from previous endpoints not consistent with this notebook.\"\n",
    "           f\"To use this notebook, please launch the endpoint again. Error: {e}.\"\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objects present in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects present in the picture: ['background', 'person']\n"
     ]
    }
   ],
   "source": [
    "print('Objects present in the picture:',image_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get color palette for visualization.\n",
    "\n",
    "We borrow the VOC pallete implementation from [GluonCV](https://cv.gluon.ai/_modules/gluoncv/utils/viz/segmentation.html#get_color_pallete)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getvocpallete(num_cls):\n",
    "    n = num_cls\n",
    "    pallete = [0]*(n*3)\n",
    "    for j in range(0, n):\n",
    "        lab = j\n",
    "        pallete[j*3+0] = 0\n",
    "        pallete[j*3+1] = 0\n",
    "        pallete[j*3+2] = 0\n",
    "        i = 0\n",
    "        while (lab > 0):\n",
    "            pallete[j*3+0] |= (((lab >> 0) & 1) << (7-i))\n",
    "            pallete[j*3+1] |= (((lab >> 1) & 1) << (7-i))\n",
    "            pallete[j*3+2] |= (((lab >> 2) & 1) << (7-i))\n",
    "            i = i + 1\n",
    "            lab >>= 3\n",
    "    return pallete\n",
    "\n",
    "pallete = getvocpallete(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add color pallete to prediction for visualization and show the predicted masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc38548aed0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "npimg = np.array(predictions)\n",
    "npimg[npimg == -1] = 255\n",
    "mask = Image.fromarray(npimg.astype('uint8'))\n",
    "mask.putpalette(pallete)\n",
    "mask.save('Mask_putput.png')\n",
    "mmask = mpimg.imread('Mask_putput.png')\n",
    "plt.imshow(mmask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pedestrian Image to JPEG so we get the pixel details\n",
    "im1 = Image.open(r'img_pedestrian.png')\n",
    "rgb_im1 = im1.convert('RGB')\n",
    "rgb_im1.save('img_pedestrian-1.jpg')\n",
    "\n",
    "# Convert the mask output to JPEG so we get the pixel details\n",
    "im2 = Image.open(r'Mask_putput.png')\n",
    "rgb_im2 = im2.convert('RGB')\n",
    "rgb_im2.save('Mask_putput-1.jpg')\n",
    "\n",
    "im1 = Image.open('img_pedestrian-1.jpg')\n",
    "pixelMap1 = im1.load()\n",
    "im2 = Image.open('Mask_putput-1.jpg')\n",
    "pixelMap2 = im2.load()\n",
    "width1, height1 = im1.size\n",
    "width2, height2 = im2.size\n",
    "\n",
    "\n",
    "# Set the height and width to the largest image\n",
    "\n",
    "if width2 > width1:\n",
    "    width = width2\n",
    "else:\n",
    "    width = width1\n",
    "\n",
    "if height2 > height1:\n",
    "    height = height2\n",
    "else:\n",
    "    height = height1\n",
    "    \n",
    "#print(width)\n",
    "#print(height)\n",
    "#for i in range(img.size[0]):    # for every col:\n",
    "#    for j in range(img.size[1]):    # For every row\n",
    "#        pixelMap[i,j] = pixelMap2[i,j] # set the colour accordingly\n",
    "for i in range(width):    # for every col:\n",
    "    for j in range(height):    # For every row\n",
    "        #print(pixelMap[i,j])\n",
    "        #print(pixelMap2[i,j])\n",
    "        R1, G1, B1 = pixelMap2[i,j]\n",
    "        if R1!=0 and G1!=0 and B1!=0:\n",
    "            pixelMap2[i,j]=pixelMap1[i,j]\n",
    "        else:\n",
    "            pixelMap2[i,j]=(0,0,0)\n",
    "            \n",
    "              \n",
    "im1.show()\n",
    "im2 = im2.save(\"Final_Pedestrians.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
